# experiment-log
llm jailbreak experiments
# LLM Jailbreak Experiments
**Adversarial Prompting Research Through Poetry**

## Overview
This repository documents ongoing research into Large Language Model (LLM) vulnerabilities using adversarial poetry and prompt engineering techniques. The work examines model misalignment, failure modes, and jailbreak vectors with particular focus on language-based attack strategies.

## Research Focus
- **Adversarial Poetry**: Exploring poetic form and structure as jailbreak mechanisms
- **Single-Turn Jailbreaks**: Developing effective one-prompt attack vectors
- **Organic Misalignment**: Documenting naturally-occurring model failures
- **Systematic Experimentation**: Building reproducible testing frameworks

## Repository Structure
- `/experiments/00-organic-findings/` - Model misalignment observed in natural conversation
- `/experiments/01-proto-experiments/` - Early experimental attempts and learning process
- `/experiments/02-systematic-experiments/` - Rigorous, reproducible experiments
- `methodology.md` - Research methods and experimental framework
- `resources.md` - References, related work, and useful links

## Background
This research began in April 2025 as informal exploration of LLM behavior and evolved into systematic study following the November 2025 publication on adversarial poetry as a jailbreak technique. The researcher brings expertise in classical philology, poetry, and linguistic analysis to AI safety research.

## Status
ðŸš§ **Active Research** - Repository currently under construction. Initial experiments forthcoming.

---

*Research conducted by KaramÃ¨yi Ã–mer | Poet & AI Safety Researcher*
